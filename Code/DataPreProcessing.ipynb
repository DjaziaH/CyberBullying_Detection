{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "#built on top of the Python programming language.\n",
    "import pandas as pd\n",
    "\n",
    "#NLTK is a leading platform for building Python programs to work with human language data.\n",
    "import nltk\n",
    "\n",
    "#Regular Expression\n",
    "import re\n",
    "\n",
    "#A specific Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, \n",
    "#like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel, SUN, MOON\n",
    "\n",
    "#This module provides access to the Unicode Character Database (UCD) which defines character properties for all\n",
    "#Unicode characters.\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call nltk.RegexpTokenizer(pattern) with pattern as r\"\\w+\" to create a tokenzier that uses pattern to split a string. \n",
    "#Call RegexpTokenizer.tokenize(text) with RegexpTokenizer as the previous result and text as a string representing a sentence\n",
    "#to return text as a list of words with punctuation's removed.( Remove punctuation and emojis)\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing methods :\n",
    "\n",
    "##  The used methods are  :\n",
    "#### remove_repeating_char ,\n",
    "#### remove_laten_char,\n",
    "#### remove_diacritics,\n",
    "#### normalize_hamza,\n",
    "#### remove_laten_char_and_diacritics,\n",
    "#### remove_url,\n",
    "#### remove_singal_letters,\n",
    "#### data_pre_processing,\n",
    "#### cleaning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    new_text = []\n",
    "    for current_word in text.split(' '):\n",
    "        new_text.append(re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", current_word))\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_laten_char(text):\n",
    "    text = ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'))\n",
    "    text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return araby.strip_tashkeel(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hamza(text):\n",
    "    return araby.normalize_hamza(text, method=\"tasheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_laten_char_and_diacritics(text):\n",
    "    return ' '.join(tokenize(text, conditions=is_arabicrange, morphs=strip_tashkeel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_singal_letters(text):\n",
    "    letters = SUN + MOON \n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in letters : \n",
    "            new_text.append(word)\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_processing(data) :\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    data = remove_url(data)\n",
    "    data = remove_laten_char(data)\n",
    "    data = remove_diacritics(data)\n",
    "    data = normalize_hamza(data)\n",
    "    data = remove_repeating_char(data)\n",
    "    data = remove_singal_letters(data)\n",
    "    #Remove punctuation and tokenize the data : \n",
    "    data = tokenizer.tokenize(data)\n",
    " \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "scrolled": false
   },
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "outputs": [],
   "source": [
    " def cleaning_data(data):\n",
    "    \n",
    "    new_comments = []\n",
    "    new_data = pd.DataFrame(columns=['author', 'authorChannelUrl', 'text','likeCount', 'publishedAt', 'offensive/non offensive', 'Algerian Dialect'])\n",
    "    for index in range(len(data)):\n",
    "        comment = data.iloc[index]\n",
    "        comment[\"text\"] = ' '.join(data_pre_processing(comment[\"text\"]))\n",
    "        if not (len( comment[\"text\"])<2 and ( not  comment[\"text\"] or len(comment[\"text\"][0]) < 2)) :\n",
    "            if(new_data.empty):\n",
    "                new_data = new_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "                new_comments.append(comment[\"text\"])\n",
    "            else:\n",
    "                if(comment[\"text\"] not in new_comments):\n",
    "                    new_data = new_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "                    new_comments.append(comment[\"text\"])\n",
    "    return new_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data from the original file :"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 17,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/YouTube Data For cyberbullying Detection in the Algerian Dialect - YouTubeDataFile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data and save it in a new file :"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 18,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-975cacb6a26b>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comment[\"text\"] = ' '.join(data_pre_processing(comment[\"text\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552\n",
      "8918\n"
     ]
    }
   ],
   "source": [
    "new_data = cleaning_data(data)\n",
    "new_data.to_csv(\"../Data/PreProcessedYouTubeDataFile.csv\")\n",
    "print(len(data))\n",
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After manual annotation of the cleaning data and removal of some insignificant author comments:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('../Data/data_1/PreProcessedYouTubeData_1.csv')\n",
    "data_2 = pd.read_csv('../Data/data_2/PreProcessedYouTubeData_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate_data :\n",
    "This function is used to separate annotated and non annotated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
=======
   "execution_count": 14,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    annotated_data = pd.DataFrame(columns=['author', 'authorChannelUrl', 'text','likeCount', 'publishedAt', 'offensive/non offensive', 'Algerian Dialect'])\n",
    "    non_annotated_data = pd.DataFrame(columns=['author', 'authorChannelUrl', 'text','likeCount', 'publishedAt', 'offensive/non offensive', 'Algerian Dialect'])\n",
    "    for index in range(len(data)):\n",
    "        comment = data.iloc[index]\n",
    "        if comment[\"offensive/non offensive\"] in [\"p\",\"n\"] :\n",
    "            annotated_data = annotated_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "        else:\n",
    "            non_annotated_data = non_annotated_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "    return annotated_data , non_annotated_data "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 19,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data_1 , non_annotated_data_1 = separate_data(data_1)\n",
    "annotated_data_2 , non_annotated_data_2 = separate_data(data_2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1 :\n",
      " - Annotated : 2580\n",
      " - Non annotated : 6338\n"
     ]
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2580"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
    }
   ],
   "source": [
    "print(\"Data 1 :\")\n",
    "print(f\" - Annotated : {len(annotated_data_1):d}\")\n",
    "print(f' - Non annotated : {len(non_annotated_data_1):d}')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 2 :\n",
      " - Annotated : 6108\n",
      " - Non annotated : 2659\n"
     ]
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6338"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
    }
   ],
   "source": [
    "print(\"Data 2 :\")\n",
    "print(f\" - Annotated : {len(annotated_data_2):d}\")\n",
    "print(f' - Non annotated : {len(non_annotated_data_2):d}')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 22,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data_1.to_csv('../Data/data_1/PreProcessedYouTubeDataFileAndAnnotated.csv')\n",
    "non_annotated_data_1.to_csv('../Data/data_1/PreProcessedYouTubeDataFileAndNonAnnotated.csv')\n",
    "\n",
    "annotated_data_2.to_csv('../Data/data_2/PreProcessedYouTubeDataFileAndAnnotated.csv')\n",
    "non_annotated_data_2.to_csv('../Data/data_2/PreProcessedYouTubeDataFileAndNonAnnotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionaries"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 23,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create dictionary that contains : (key= word  :  weight=avgOfSeverities)\n",
    "\n",
    "def create_dictionary_with_severity():\n",
    "    df = pd.read_csv('../Data/bad_words.csv')\n",
    "    dictionary = {}\n",
    "    for i in df.index:\n",
    "        data = (df.loc[i])\n",
    "        key = ' '.join(data_pre_processing(data[\"bad_word\"]))\n",
    "        if key in dictionary:\n",
    "            dictionary[key] = (dictionary[key] + data[\"severity\"]) / 2\n",
    "        else :\n",
    "            dictionary[key] = data[\"severity\"]\n",
    "    return dictionary \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 24,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that contains : (key= word  :  frequency= number of time it occurs)\n",
    "\n",
    "def create_dictionary_with_frequency():\n",
    "    df = pd.read_csv('../Data/bad_words.csv')\n",
    "    dictionary = {}\n",
    "    for i in df.index:\n",
    "        data = (df.loc[i])\n",
    "        key = ' '.join(data_pre_processing(data[\"bad_word\"]))\n",
    "        if key in dictionary:\n",
    "            dictionary[key] = (dictionary[key] + 1)\n",
    "        else :\n",
    "            dictionary[key] = 1\n",
    "    return dictionary "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 25,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that contains : (key= (word,classe)  :  weight=avgOfSeverities)\n",
    "\n",
    "def create_dictionaryClasses():\n",
    "    df = pd.read_csv('../Data/bad_words.csv')\n",
    "    dictionaryِClasses = {}\n",
    "    classes = [\"سب\" , \"سخرية\" , \"تحرش\" , \"تهديد\"]\n",
    "\n",
    "    for i in df.index:\n",
    "        data = (df.loc[i])\n",
    "        for class_ in classes : \n",
    "            key = (' '.join(data_pre_processing(data[\"bad_word\"])), class_ )\n",
    "            if key in dictionaryِClasses:\n",
    "                if(data[\"classification\"] == class_):\n",
    "                    dictionaryِClasses[key] = (dictionaryِClasses[key] + data[\"severity\"]) / 2\n",
    "            else :\n",
    "                if(data[\"classification\"] == class_):\n",
    "                    dictionaryِClasses[key] = data[\"severity\"]\n",
    "    return dictionaryِClasses"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 26,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_severity = create_dictionary_with_severity()\n",
    "dic_classes = create_dictionaryClasses()\n",
    "dic_frequency = create_dictionary_with_frequency()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 27,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(dic_severity.items()),columns = ['bad_word','severity'] ).to_csv('../Data/dictionaries/dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 28,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(dic_classes.items()),columns = ['bad_word','severity'] ).to_csv('../Data/dictionaries/dictionaryWithClasses.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 29,
>>>>>>> 20d7534fa0ea3bad2e6f02fae9792e7aeac462e6
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(dic_frequency.items()),columns = ['bad_word','frequency'] ).to_csv('../Data/dictionaries/dictionaryWithFrequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
