{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "#built on top of the Python programming language.\n",
    "import pandas as pd\n",
    "\n",
    "#NLTK is a leading platform for building Python programs to work with human language data.\n",
    "import nltk\n",
    "\n",
    "#Regular Expression\n",
    "import re\n",
    "\n",
    "#A specific Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, \n",
    "#like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n",
    "\n",
    "#This module provides access to the Unicode Character Database (UCD) which defines character properties for all\n",
    "#Unicode characters.\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call nltk.RegexpTokenizer(pattern) with pattern as r\"\\w+\" to create a tokenzier that uses pattern to split a string. \n",
    "#Call RegexpTokenizer.tokenize(text) with RegexpTokenizer as the previous result and text as a string representing a sentence\n",
    "#to return text as a list of words with punctuation's removed.( Remove punctuation and emojis)\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    new_text = []\n",
    "    for current_word in text.split(' '):\n",
    "        if current_word not in  ['الله','والله']:\n",
    "            new_text.append(re.sub(r'(.)\\1+', r'\\1', current_word))\n",
    "        else : \n",
    "            new_text.append(current_word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_laten_char(text):\n",
    "    text = ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'))\n",
    "    text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return araby.strip_tashkeel(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hamza(text):\n",
    "    return araby.normalize_hamza(text, method=\"tasheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_laten_char_and_diacritics(text):\n",
    "    return ' '.join(tokenize(text, conditions=is_arabicrange, morphs=strip_tashkeel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_processing(data) :\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    data = remove_laten_char(data)\n",
    "    data = remove_diacritics(data)\n",
    "    data = normalize_hamza(data)\n",
    "    data = remove_repeating_char(data)\n",
    "    #Remove punctuation and tokenize the data : \n",
    "    data = tokenizer.tokenize(data)\n",
    " \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cleaning_data(data):\n",
    "    \n",
    "    new_comments = []\n",
    "    new_data = pd.DataFrame(columns=['author', 'authorChannelUrl', 'text','likeCount', 'publishedAt', 'offensive/non offensive', 'Algerian Dialect'])\n",
    "    for index in range(len(data)):\n",
    "        comment = data.iloc[index]\n",
    "        comment[\"text\"] = ' '.join(data_pre_processing(comment[\"text\"]))\n",
    "        if not (len( comment[\"text\"])<2 and ( not  comment[\"text\"] or len(comment[\"text\"][0]) < 2)) :\n",
    "            if(new_data.empty):\n",
    "                new_data = new_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "                new_comments.append(comment[\"text\"])\n",
    "            else:\n",
    "                if(comment[\"text\"] not in new_comments):\n",
    "                    new_data = new_data.append({'author':comment[\"author\"] , 'authorChannelUrl':comment[\"authorChannelUrl\"] , 'text':comment[\"text\"] ,'likeCount':comment[\"likeCount\"] , 'publishedAt':comment[\"publishedAt\"] , 'offensive/non offensive':comment[\"offensive/non offensive\"] , 'Algerian Dialect':comment[\"Algerian Dialect\"] }, ignore_index=True)\n",
    "                    new_comments.append(comment[\"text\"])\n",
    "    return new_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/YouTube Data For cyberbullying Detection in the Algerian Dialect - YouTubeDataFile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552\n",
      "8931\n"
     ]
    }
   ],
   "source": [
    "new_data = cleaning_data(data)\n",
    "new_data.to_csv(\"../Data/PreProcessedYouTubeDataFile.csv\")\n",
    "print(len(data))\n",
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create dictionary that contains : (key= word  :  weight=avgOfSeverities)\n",
    "\n",
    "def create_dictionary():\n",
    "    df = pd.read_csv('../Data/bad_words.csv')\n",
    "    dictionary = {}\n",
    "    for i in df.index:\n",
    "        data = (df.loc[i])\n",
    "        key = ' '.join(data_pre_processing(data[\"bad_word\"]))\n",
    "        if key in dictionary:\n",
    "            dictionary[key] = (dictionary[key] + data[\"severity\"]) / 2\n",
    "        else :\n",
    "            dictionary[key] = data[\"severity\"]\n",
    "    return dictionary \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that contains : (key= (word,classe)  :  weight=avgOfSeverities)\n",
    "\n",
    "def create_dictionaryClasses():\n",
    "    df = pd.read_csv('../Data/bad_words.csv')\n",
    "    dictionaryِClasses = {}\n",
    "    classes = [\"سب\" , \"سخرية\" , \"تحرش\" , \"تهديد\"]\n",
    "\n",
    "    for i in df.index:\n",
    "        data = (df.loc[i])\n",
    "        for class_ in classes : \n",
    "            key = (' '.join(data_pre_processing(data[\"bad_word\"])), class_ )\n",
    "            if key in dictionaryِClasses:\n",
    "                if(data[\"classification\"] == class_):\n",
    "                    dictionaryِClasses[key] = (dictionaryِClasses[key] + data[\"severity\"]) / 2\n",
    "            else :\n",
    "                if(data[\"classification\"] == class_):\n",
    "                    dictionaryِClasses[key] = data[\"severity\"]\n",
    "    return dictionaryِClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = create_dictionary()\n",
    "dic_clas = create_dictionaryِClasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(dic.items()),columns = ['bad_word','severity'] ).to_csv('../Data/dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(dic_clas.items()),columns = ['bad_word','severity'] ).to_csv('../Data/dictionaryWithClasses.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
