{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the annotated data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('../Data/data_1/PreProcessedYoutubeDataFileAndAnnotated.csv')\n",
    "data_2 = pd.read_csv('../Data/data_2/PreProcessedYoutubeDataFileAndAnnotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_1 = data_1.iloc[:,1:]\n",
    "data_2 = data_2.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Comments of the Algerian Dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AD_comments(data):\n",
    "    for index in range(len(data)) : \n",
    "        if data.iloc[index][\"Algerian Dialect\"] == \"n\":\n",
    "            data.drop(data.index[index], inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = get_AD_comments(data_1)\n",
    "data_2 = get_AD_comments(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_1 : 2567\n",
      "Data_2 : 6068\n"
     ]
    }
   ],
   "source": [
    "print(f'Data_1 : {len(data_1)}')\n",
    "print(f'Data_2 : {len(data_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save only Algerian dialect comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_csv('../Data/data_1/PreProcessedYoutubeDataFileAndAnnotated.csv')\n",
    "data_2.to_csv('../Data/data_2/PreProcessedYoutubeDataFileAndAnnotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Offensive Comments from the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Return Offensive comments from the data \n",
    "def get_offensive_comments(data):\n",
    "    \n",
    "    offensiveComment = pd.DataFrame(columns=['text'])\n",
    "    for index in range(len(data)):\n",
    "        if data.iloc[index][\"offensive/non offensive\"] == \"p\":\n",
    "            offensiveComment = offensiveComment.append({\"text\" : data.iloc[index][\"text\"]},ignore_index=True)\n",
    "    return offensiveComment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensiveComment_1 = get_offensive_comments(data_1)\n",
    "offensiveComment_2 = get_offensive_comments(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select comments with a length of 4 or less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1_1 = []\n",
    "list_1_2 = []\n",
    "list_1_3 = []\n",
    "list_1_4 = []\n",
    "\n",
    "\n",
    "list_2_1 = []\n",
    "list_2_2 = []\n",
    "list_2_3 = []\n",
    "list_2_4 = []\n",
    "\n",
    "for index in range(len(offensiveComment_1)) : \n",
    "    comment = offensiveComment_1.iloc[index][\"text\"]\n",
    "    commentLength = len(comment.split(' '))\n",
    "\n",
    "    if(commentLength == 1) :\n",
    "        list_1_1.append(comment)\n",
    "    elif(commentLength == 2) : \n",
    "        list_1_2.append(comment)\n",
    "    elif(commentLength == 3) : \n",
    "        list_1_3.append(comment)\n",
    "    elif(commentLength == 4) : \n",
    "        list_1_4.append(comment)\n",
    "        \n",
    "for index in range(len(offensiveComment_2)) : \n",
    "    comment = offensiveComment_2.iloc[index][\"text\"]\n",
    "    commentLength = len(comment.split(' '))\n",
    "\n",
    "    if(commentLength == 1) :\n",
    "        list_2_1.append(comment)\n",
    "    elif(commentLength == 2) : \n",
    "        list_2_2.append(comment)\n",
    "    elif(commentLength == 3) : \n",
    "        list_2_3.append(comment)\n",
    "    elif(commentLength == 4) : \n",
    "        list_2_4.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1 : 389\n",
      "List 2 : 704\n"
     ]
    }
   ],
   "source": [
    "new_bad_words_1 = list_1_1 + list_1_2 + list_1_3 + list_1_4\n",
    "print(f'List 1 : {len(new_bad_words_1)}')\n",
    "\n",
    "new_bad_words_2 = list_2_1 + list_2_2 + list_2_3 + list_2_4\n",
    "print(f'List 2 : {len(new_bad_words_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(new_bad_words_1)\n",
    "df_2 = pd.DataFrame(new_bad_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns = ['bad_word']\n",
    "df_2.columns = ['bad_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(\"../Data/data_1/new_bad_words.csv\")\n",
    "df_2.to_csv(\"../Data/data_2/new_bad_words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After getting the new lists of bad words :\n",
    "we have deleted names of person from sentences, and removed repeated words in the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = pd.read_csv(\"../Data/data_1/new_bad_words_1.csv\")\n",
    "df_1_2 = pd.read_csv(\"../Data/data_2/new_bad_words_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New List 1 : 383\n",
      "New List 2 : 702\n"
     ]
    }
   ],
   "source": [
    "print(f'New List 1 : {len(df_1_1)}')\n",
    "print(f'New List 2 : {len(df_1_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the list to the dictionary :\n",
    "we have checked if these are really bad words and are deleting some unnecessary words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the new words to the dictionary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_words_to_dictionary_with_frequency(new_list):\n",
    "    dictionary = pd.read_csv('../Data/dictionaries/dictionaryWithFrequency.csv')\n",
    "    dictionary = dict(zip(dictionary[\"bad_word\"], dictionary[\"frequency\"]))\n",
    "\n",
    "    new_bad_words = new_list[[\"bad_word\"]]\n",
    "\n",
    "    for i in new_bad_words.index:\n",
    "        new_word = (new_bad_words.loc[i][\"bad_word\"])\n",
    "        if new_word in dictionary :\n",
    "            dictionary[new_word] = (dictionary[new_word] + 1)\n",
    "        else :\n",
    "            dictionary[new_word] = 1\n",
    "    return dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv('../Data/dictionaries/dictionaryWithFrequency.csv')\n",
    "dictionary = dict(zip(dictionary[\"bad_word\"], dictionary[\"frequency\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_1 = df_1_1.iloc[:,1:]\n",
    "df_1_2 = df_1_2.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict_1 = adding_words_to_dictionary_with_frequency(df_1_1)\n",
    "new_dict_2 = adding_words_to_dictionary_with_frequency(df_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary : 218\n",
      "New dictionary 1 : 585\n",
      "New dictionary 2 : 897\n"
     ]
    }
   ],
   "source": [
    "print(f'Dictionary : {len(dictionary)}')\n",
    "print(f'New dictionary 1 : {len(new_dict_1)}')\n",
    "print(f'New dictionary 2 : {len(new_dict_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save new dictionaries : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(new_dict_1.items()),columns = ['bad_word','frequency'] ).to_csv('../Data/dictionaries/dictionary_with_new_bad_words_1.csv')\n",
    "pd.DataFrame(list(new_dict_2.items()),columns = ['bad_word','frequency'] ).to_csv('../Data/dictionaries/dictionary_with_new_bad_words_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
